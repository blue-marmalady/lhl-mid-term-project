{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Delay Predictive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Imports\n",
    "\n",
    "# ML Standards\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import MiniBatchSparsePCA\n",
    "\n",
    "# Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRFClassifier\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, classification_report \n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.stats import fisher_exact\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data (the file was initially processed in data-collection)\n",
    "flight_delay = pd.read_csv(\"flight_information.csv\")\n",
    "\n",
    "#Test data\n",
    "# flight_delay = pd.read_csv(\"flight_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fl_date</th>\n",
       "      <th>mkt_unique_carrier</th>\n",
       "      <th>branded_code_share</th>\n",
       "      <th>mkt_carrier</th>\n",
       "      <th>mkt_carrier_fl_num</th>\n",
       "      <th>op_unique_carrier</th>\n",
       "      <th>tail_num</th>\n",
       "      <th>op_carrier_fl_num</th>\n",
       "      <th>origin_airport_id</th>\n",
       "      <th>...</th>\n",
       "      <th>distance</th>\n",
       "      <th>carrier_delay</th>\n",
       "      <th>weather_delay</th>\n",
       "      <th>nas_delay</th>\n",
       "      <th>security_delay</th>\n",
       "      <th>late_aircraft_delay</th>\n",
       "      <th>first_dep_time</th>\n",
       "      <th>total_add_gtime</th>\n",
       "      <th>longest_add_gtime</th>\n",
       "      <th>no_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-07-25</td>\n",
       "      <td>B6</td>\n",
       "      <td>B6</td>\n",
       "      <td>B6</td>\n",
       "      <td>695</td>\n",
       "      <td>B6</td>\n",
       "      <td>N239JB</td>\n",
       "      <td>695</td>\n",
       "      <td>12197</td>\n",
       "      <td>...</td>\n",
       "      <td>972.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-02-16</td>\n",
       "      <td>UA</td>\n",
       "      <td>UA_CODESHARE</td>\n",
       "      <td>UA</td>\n",
       "      <td>5617</td>\n",
       "      <td>OO</td>\n",
       "      <td>N160SY</td>\n",
       "      <td>5617</td>\n",
       "      <td>11298</td>\n",
       "      <td>...</td>\n",
       "      <td>801.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-02-28</td>\n",
       "      <td>DL</td>\n",
       "      <td>DL</td>\n",
       "      <td>DL</td>\n",
       "      <td>1730</td>\n",
       "      <td>DL</td>\n",
       "      <td>N915AT</td>\n",
       "      <td>1730</td>\n",
       "      <td>13930</td>\n",
       "      <td>...</td>\n",
       "      <td>606.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-10-28</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA_CODESHARE</td>\n",
       "      <td>AA</td>\n",
       "      <td>4688</td>\n",
       "      <td>YX</td>\n",
       "      <td>N104HQ</td>\n",
       "      <td>4688</td>\n",
       "      <td>12124</td>\n",
       "      <td>...</td>\n",
       "      <td>207.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-27</td>\n",
       "      <td>WN</td>\n",
       "      <td>WN</td>\n",
       "      <td>WN</td>\n",
       "      <td>1775</td>\n",
       "      <td>WN</td>\n",
       "      <td>N8632A</td>\n",
       "      <td>1775</td>\n",
       "      <td>15016</td>\n",
       "      <td>...</td>\n",
       "      <td>687.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>199995</td>\n",
       "      <td>2019-05-18</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>1555</td>\n",
       "      <td>AA</td>\n",
       "      <td>N562UW</td>\n",
       "      <td>1555</td>\n",
       "      <td>12889</td>\n",
       "      <td>...</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>199996</td>\n",
       "      <td>2019-06-24</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS_CODESHARE</td>\n",
       "      <td>AS</td>\n",
       "      <td>3449</td>\n",
       "      <td>OO</td>\n",
       "      <td>N196SY</td>\n",
       "      <td>3449</td>\n",
       "      <td>14869</td>\n",
       "      <td>...</td>\n",
       "      <td>599.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>199997</td>\n",
       "      <td>2019-01-13</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>858</td>\n",
       "      <td>NK</td>\n",
       "      <td>N507NK</td>\n",
       "      <td>858</td>\n",
       "      <td>14679</td>\n",
       "      <td>...</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>199998</td>\n",
       "      <td>2019-04-08</td>\n",
       "      <td>UA</td>\n",
       "      <td>UA_CODESHARE</td>\n",
       "      <td>UA</td>\n",
       "      <td>5430</td>\n",
       "      <td>OO</td>\n",
       "      <td>N124SY</td>\n",
       "      <td>5430</td>\n",
       "      <td>14771</td>\n",
       "      <td>...</td>\n",
       "      <td>599.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>199999</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>2663</td>\n",
       "      <td>AA</td>\n",
       "      <td>N7547A</td>\n",
       "      <td>2663</td>\n",
       "      <td>11298</td>\n",
       "      <td>...</td>\n",
       "      <td>624.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0     fl_date mkt_unique_carrier branded_code_share  \\\n",
       "0                0  2018-07-25                 B6                 B6   \n",
       "1                1  2019-02-16                 UA       UA_CODESHARE   \n",
       "2                2  2019-02-28                 DL                 DL   \n",
       "3                3  2018-10-28                 AA       AA_CODESHARE   \n",
       "4                4  2018-04-27                 WN                 WN   \n",
       "...            ...         ...                ...                ...   \n",
       "199995      199995  2019-05-18                 AA                 AA   \n",
       "199996      199996  2019-06-24                 AS       AS_CODESHARE   \n",
       "199997      199997  2019-01-13                 NK                 NK   \n",
       "199998      199998  2019-04-08                 UA       UA_CODESHARE   \n",
       "199999      199999  2018-02-05                 AA                 AA   \n",
       "\n",
       "       mkt_carrier  mkt_carrier_fl_num op_unique_carrier tail_num  \\\n",
       "0               B6                 695                B6   N239JB   \n",
       "1               UA                5617                OO   N160SY   \n",
       "2               DL                1730                DL   N915AT   \n",
       "3               AA                4688                YX   N104HQ   \n",
       "4               WN                1775                WN   N8632A   \n",
       "...            ...                 ...               ...      ...   \n",
       "199995          AA                1555                AA   N562UW   \n",
       "199996          AS                3449                OO   N196SY   \n",
       "199997          NK                 858                NK   N507NK   \n",
       "199998          UA                5430                OO   N124SY   \n",
       "199999          AA                2663                AA   N7547A   \n",
       "\n",
       "        op_carrier_fl_num  origin_airport_id  ... distance carrier_delay  \\\n",
       "0                     695              12197  ...    972.0          10.0   \n",
       "1                    5617              11298  ...    801.0           NaN   \n",
       "2                    1730              13930  ...    606.0           NaN   \n",
       "3                    4688              12124  ...    207.0           NaN   \n",
       "4                    1775              15016  ...    687.0           NaN   \n",
       "...                   ...                ...  ...      ...           ...   \n",
       "199995               1555              12889  ...    236.0           0.0   \n",
       "199996               3449              14869  ...    599.0           NaN   \n",
       "199997                858              14679  ...   1303.0           NaN   \n",
       "199998               5430              14771  ...    599.0           NaN   \n",
       "199999               2663              11298  ...    624.0           NaN   \n",
       "\n",
       "        weather_delay nas_delay security_delay  late_aircraft_delay  \\\n",
       "0                 0.0       4.0            0.0                 31.0   \n",
       "1                 NaN       NaN            NaN                  NaN   \n",
       "2                 NaN       NaN            NaN                  NaN   \n",
       "3                 NaN       NaN            NaN                  NaN   \n",
       "4                 NaN       NaN            NaN                  NaN   \n",
       "...               ...       ...            ...                  ...   \n",
       "199995            0.0       0.0            0.0                 23.0   \n",
       "199996            NaN       NaN            NaN                  NaN   \n",
       "199997            NaN       NaN            NaN                  NaN   \n",
       "199998            NaN       NaN            NaN                  NaN   \n",
       "199999            NaN       NaN            NaN                  NaN   \n",
       "\n",
       "        first_dep_time  total_add_gtime  longest_add_gtime  no_name  \n",
       "0                  NaN              NaN                NaN      NaN  \n",
       "1                  NaN              NaN                NaN      NaN  \n",
       "2                  NaN              NaN                NaN      NaN  \n",
       "3                  NaN              NaN                NaN      NaN  \n",
       "4                  NaN              NaN                NaN      NaN  \n",
       "...                ...              ...                ...      ...  \n",
       "199995             NaN              NaN                NaN      NaN  \n",
       "199996             NaN              NaN                NaN      NaN  \n",
       "199997             NaN              NaN                NaN      NaN  \n",
       "199998             NaN              NaN                NaN      NaN  \n",
       "199999             NaN              NaN                NaN      NaN  \n",
       "\n",
       "[200000 rows x 43 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we're working with\n",
    "flight_delay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'fl_date', 'mkt_unique_carrier', 'branded_code_share',\n",
       "       'mkt_carrier', 'mkt_carrier_fl_num', 'op_unique_carrier', 'tail_num',\n",
       "       'op_carrier_fl_num', 'origin_airport_id', 'origin', 'origin_city_name',\n",
       "       'dest_airport_id', 'dest', 'dest_city_name', 'crs_dep_time', 'dep_time',\n",
       "       'dep_delay', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in',\n",
       "       'crs_arr_time', 'arr_time', 'arr_delay', 'cancelled',\n",
       "       'cancellation_code', 'diverted', 'dup', 'crs_elapsed_time',\n",
       "       'actual_elapsed_time', 'air_time', 'flights', 'distance',\n",
       "       'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay',\n",
       "       'late_aircraft_delay', 'first_dep_time', 'total_add_gtime',\n",
       "       'longest_add_gtime', 'no_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_delay.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract some useful features here (and eliminate non useful features).\n",
    "\n",
    "The features that will have primary importance are:\n",
    "\n",
    "- Time of day and time of year\n",
    "- Departing and arrival airports\n",
    "- Airlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: not required for test data\n",
    "\n",
    "# # Cleaning things: flights should take longer than 0 minutes\n",
    "#  flight_delay = flight_delay[flight_delay['air_time']>0]\n",
    "\n",
    "# #Don't consider diverted or cancelled flights\n",
    "#  flight_delay.drop(flight_delay[(flight_delay['cancelled']==1) | (flight_delay['diverted']==1)].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract desired features into new df\n",
    "model_df = flight_delay[['fl_date', 'mkt_unique_carrier', 'crs_dep_time', 'crs_arr_time', 'origin', 'dest', 'arr_delay', 'distance']]\n",
    "# model_df = flight_delay[['fl_date', 'mkt_unique_carrier', 'crs_dep_time', 'crs_arr_time', 'origin', 'dest', 'distance']]\n",
    "\n",
    "#Only consider flights that are at no more than 60 minutes early (earlier flights may simply be misclassified )\n",
    "model_df = model_df[model_df['arr_delay']>-60]\n",
    "\n",
    "# As this is will be a classification, we will have two broad categories: not delayed and delayed.\n",
    "# Classify all early flights as arriving on time (the 'not delayed' category) \n",
    "model_df['arr_delay'] = model_df['arr_delay'].where(model_df['arr_delay'] > 0, 0)\n",
    "model_df = model_df.fillna(0) # For missing values\n",
    "\n",
    "### Categorizing hours and time of day of flights\n",
    "# For departure/arrival times, extract only the hour of departure/arrival\n",
    "model_df['dep_hr'] = (model_df['crs_dep_time'] // 100).astype('str')\n",
    "model_df['arr_hr'] = (model_df['crs_arr_time'] // 100).astype('str')\n",
    "model_df = model_df.drop(columns=['crs_dep_time', 'crs_arr_time'])\n",
    "\n",
    "# Aggregate based on time of day (morning, afternoon, evening)\n",
    "model_df['dep_time_of_day'] = model_df['dep_hr'] # just create the column and fill to start\n",
    "model_df['arr_time_of_day'] = model_df['arr_hr'] \n",
    "model_df['dep_hr'] = pd.to_numeric(model_df['dep_hr']) # Needs to be an integer for our filtering below\n",
    "model_df['arr_hr'] = pd.to_numeric(model_df['arr_hr']) \n",
    "\n",
    "# Assigning categories\n",
    "model_df.loc[(model_df['dep_hr'] >=5) & (model_df['dep_hr'] <12), 'dep_time_of_day'] = 'morn'\n",
    "model_df.loc[(model_df['dep_hr'] >=12) & (model_df['dep_hr'] <17), 'dep_time_of_day'] = 'aft'\n",
    "model_df.loc[(model_df['dep_hr'] >=17) | (model_df['dep_hr'] <5), 'dep_time_of_day'] = 'eve'\n",
    "\n",
    "model_df.loc[(model_df['arr_hr'] >=5) & (model_df['arr_hr'] <12), 'arr_time_of_day'] = 'morn'\n",
    "model_df.loc[(model_df['arr_hr'] >=12) & (model_df['arr_hr'] <17), 'arr_time_of_day'] = 'aft'\n",
    "model_df.loc[(model_df['arr_hr'] >=17) |  (model_df['arr_hr'] <5), 'arr_time_of_day'] = 'eve'\n",
    "\n",
    "\n",
    "### Categorizing day, month and time of year of flights \n",
    "# Extract month and day of week from the flight dates \n",
    "model_df['month'] = pd.DatetimeIndex(model_df['fl_date']).month\n",
    "model_df['weekday'] = pd.DatetimeIndex(model_df['fl_date']).weekday\n",
    "model_df = model_df.drop(columns=['fl_date'])\n",
    "\n",
    "# Aggregate into season, based on months\n",
    "model_df['season'] = model_df['month'] # just create column and fill to start\n",
    "model_df['month'] = pd.to_numeric(model_df['month'] ) # Needs to be an integer for our filtering below\n",
    "\n",
    "#Assigning categories\n",
    "model_df.loc[(model_df['month'] >= 3) & (model_df['month'] < 6), 'season' ] = 'spr'\n",
    "model_df.loc[(model_df['month'] >= 6) & (model_df['month'] < 9), 'season'] = 'sum'\n",
    "model_df.loc[(model_df['month'] >= 9) & (model_df['month'] < 12), 'season'] = 'aut'\n",
    "model_df.loc[(model_df['month'] == 12) | (model_df['month'] < 3), 'season'] = 'win'\n",
    "\n",
    "# and make ready for categorical\n",
    "model_df['month'] = model_df['month'].replace({1: \"Jan\", 2: \"Feb\", 3: \"Mar\", 4: \"Apr\",\n",
    "                            5: \"May\", 6: \"Jun\", 7: \"Jul\", 8: \"Aug\", 9: \"Sep\", 10: \"Oct\", 11: \"Nov\", 12: \"Dec\"})\n",
    "model_df['weekday'] = model_df['weekday'].replace({0: \"Mon\", 1: \"Tue\", 2: \"Wed\", 3: \"Thu\",\n",
    "                            4: \"Fri\", 5: \"Sat\", 6: \"Sun\"})\n",
    "\n",
    "\n",
    "#### While we're here, let's categorize airlines into size: \"large\" and \"small\" \n",
    "# (Airlines chosen based on exploratory analysis)\n",
    "model_df['carrier_size'] = model_df['mkt_unique_carrier'] # just create the column to start\n",
    "model_df.loc[(model_df['mkt_unique_carrier'] == 'UA') | (model_df['mkt_unique_carrier'] == 'AA') | \\\n",
    "            (model_df['mkt_unique_carrier'] == 'WN') | (model_df['mkt_unique_carrier'] == 'DL') | \\\n",
    "            (model_df['mkt_unique_carrier'] == 'AS'), 'carrier_size'] = 'large' \n",
    "\n",
    "model_df.loc[(model_df['mkt_unique_carrier'] == 'VX') | (model_df['mkt_unique_carrier'] == 'B6') | \\\n",
    "            (model_df['mkt_unique_carrier'] == 'HA') | (model_df['mkt_unique_carrier'] == 'F9') | \\\n",
    "            (model_df['mkt_unique_carrier'] == 'G4')| (model_df['mkt_unique_carrier'] == 'NK'), 'carrier_size'] = 'small'\n",
    "\n",
    "#If needed, we can also create a 'medium' category: F9, B6, NK, AS\n",
    " \n",
    "\n",
    "#### You know what, let's categorize flight distance too \n",
    "# Categories again chosen based on exploratory analysis\n",
    "model_df['distance_cat'] = model_df['distance']\n",
    "model_df['distance'] = pd.to_numeric(model_df['distance'] ) # Needs to be an integer for our filtering below\n",
    "\n",
    "model_df.loc[(model_df['distance'] < 400) , 'distance_cat' ] = 'short'\n",
    "model_df.loc[(model_df['distance'] >= 400) & (model_df['distance'] < 800), 'distance_cat'] = 'med'\n",
    "model_df.loc[(model_df['distance'] >= 800) , 'distance_cat' ] = 'long'\n",
    "model_df.drop('distance', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mkt_unique_carrier</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>dep_hr</th>\n",
       "      <th>arr_hr</th>\n",
       "      <th>dep_time_of_day</th>\n",
       "      <th>arr_time_of_day</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>season</th>\n",
       "      <th>carrier_size</th>\n",
       "      <th>distance_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B6</td>\n",
       "      <td>HPN</td>\n",
       "      <td>MCO</td>\n",
       "      <td>45.0</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>eve</td>\n",
       "      <td>eve</td>\n",
       "      <td>Jul</td>\n",
       "      <td>Wed</td>\n",
       "      <td>sum</td>\n",
       "      <td>small</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UA</td>\n",
       "      <td>DFW</td>\n",
       "      <td>ORD</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>aft</td>\n",
       "      <td>eve</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Sat</td>\n",
       "      <td>win</td>\n",
       "      <td>large</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DL</td>\n",
       "      <td>ORD</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>eve</td>\n",
       "      <td>eve</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Thu</td>\n",
       "      <td>win</td>\n",
       "      <td>large</td>\n",
       "      <td>med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA</td>\n",
       "      <td>HHH</td>\n",
       "      <td>CLT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>morn</td>\n",
       "      <td>morn</td>\n",
       "      <td>Oct</td>\n",
       "      <td>Sun</td>\n",
       "      <td>aut</td>\n",
       "      <td>large</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WN</td>\n",
       "      <td>STL</td>\n",
       "      <td>HOU</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>eve</td>\n",
       "      <td>eve</td>\n",
       "      <td>Apr</td>\n",
       "      <td>Fri</td>\n",
       "      <td>spr</td>\n",
       "      <td>large</td>\n",
       "      <td>med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>AA</td>\n",
       "      <td>LAS</td>\n",
       "      <td>LAX</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>aft</td>\n",
       "      <td>aft</td>\n",
       "      <td>May</td>\n",
       "      <td>Sat</td>\n",
       "      <td>spr</td>\n",
       "      <td>large</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>AS</td>\n",
       "      <td>SLC</td>\n",
       "      <td>SFO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>morn</td>\n",
       "      <td>morn</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Mon</td>\n",
       "      <td>sum</td>\n",
       "      <td>large</td>\n",
       "      <td>med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>NK</td>\n",
       "      <td>SAN</td>\n",
       "      <td>IAH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>morn</td>\n",
       "      <td>aft</td>\n",
       "      <td>Jan</td>\n",
       "      <td>Sun</td>\n",
       "      <td>win</td>\n",
       "      <td>small</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>UA</td>\n",
       "      <td>SFO</td>\n",
       "      <td>SLC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>aft</td>\n",
       "      <td>eve</td>\n",
       "      <td>Apr</td>\n",
       "      <td>Mon</td>\n",
       "      <td>spr</td>\n",
       "      <td>large</td>\n",
       "      <td>med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>AA</td>\n",
       "      <td>DFW</td>\n",
       "      <td>DSM</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>eve</td>\n",
       "      <td>eve</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Mon</td>\n",
       "      <td>win</td>\n",
       "      <td>large</td>\n",
       "      <td>med</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199963 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mkt_unique_carrier origin dest  arr_delay  dep_hr  arr_hr  \\\n",
       "0                      B6    HPN  MCO       45.0      20      23   \n",
       "1                      UA    DFW  ORD        7.0      14      17   \n",
       "2                      DL    ORD  ATL        0.0      17      20   \n",
       "3                      AA    HHH  CLT        0.0       6       8   \n",
       "4                      WN    STL  HOU        0.0      21      23   \n",
       "...                   ...    ...  ...        ...     ...     ...   \n",
       "199995                 AA    LAS  LAX       23.0      14      16   \n",
       "199996                 AS    SLC  SFO        0.0       7       8   \n",
       "199997                 NK    SAN  IAH        0.0       7      12   \n",
       "199998                 UA    SFO  SLC        0.0      15      18   \n",
       "199999                 AA    DFW  DSM       12.0      20      22   \n",
       "\n",
       "       dep_time_of_day arr_time_of_day month weekday season carrier_size  \\\n",
       "0                  eve             eve   Jul     Wed    sum        small   \n",
       "1                  aft             eve   Feb     Sat    win        large   \n",
       "2                  eve             eve   Feb     Thu    win        large   \n",
       "3                 morn            morn   Oct     Sun    aut        large   \n",
       "4                  eve             eve   Apr     Fri    spr        large   \n",
       "...                ...             ...   ...     ...    ...          ...   \n",
       "199995             aft             aft   May     Sat    spr        large   \n",
       "199996            morn            morn   Jun     Mon    sum        large   \n",
       "199997            morn             aft   Jan     Sun    win        small   \n",
       "199998             aft             eve   Apr     Mon    spr        large   \n",
       "199999             eve             eve   Feb     Mon    win        large   \n",
       "\n",
       "       distance_cat  \n",
       "0              long  \n",
       "1              long  \n",
       "2               med  \n",
       "3             short  \n",
       "4               med  \n",
       "...             ...  \n",
       "199995        short  \n",
       "199996          med  \n",
       "199997         long  \n",
       "199998          med  \n",
       "199999          med  \n",
       "\n",
       "[199963 rows x 13 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection / Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Sparse PCA (since our matrix is very sparse); Mini Batch runs quicker, but sacrifices accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest classifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it with all of our features to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_target(df):\n",
    "    '''Creates feature and target arrays ready for processing:\n",
    "    Params:\n",
    "        - df: expected dataframe \n",
    "    Returns:\n",
    "        - X: array of features (turns categorical features into dummy variables)\n",
    "        - X_columns: array of feature names\n",
    "        - y: array of targets (in this case, arrival delays)\n",
    "    '''\n",
    "    # Extract target variable\n",
    "    # Create labels for data (Rather than treat it as continuous)\n",
    "\n",
    "    y_clas = df[['arr_delay']]\n",
    "    y_clas.loc[( (y_clas['arr_delay']  >= 0) & (y_clas['arr_delay'] <15)), 'arr_delay'] = 0\n",
    "    y_clas.loc[(y_clas['arr_delay']  >= 15), 'arr_delay'] = 1\n",
    "    df = df.drop(columns=['arr_delay'])\n",
    "\n",
    "    # ...and create categorical features\n",
    "    X = pd.get_dummies(df)\n",
    "    X_columns = X.columns\n",
    "    return X, X_columns, y_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcjb/anaconda3/envs/lhl_env/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/home/marcjb/anaconda3/envs/lhl_env/lib/python3.9/site-packages/pandas/core/indexing.py:723: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    }
   ],
   "source": [
    "feature_df, X_columns, y_clas = feature_target(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might make sense to use the same split for all the analysis...\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(feature_df, y_clas, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Preamble\n",
    "\n",
    "We want to be able to evaluate how our model is performing; looking at accuracy (*i.e.* % of correct predictions over all results) is not necessarily the best way to evaluate our model. For example, let's see how many delays there are in our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arr_delay    0.19052\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_clas.sum()/y_clas.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about 19% of the flights in our test data is delayed. Well, we could just say *all* of our flights are delayed and get 81% accuracy! Which is not ideal; we would like to be able to predict at least *some* of the delays. So how do we assess whether our model is able to do that?\n",
    "\n",
    "We can use a confusion matrix (which tells us [precision and recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)), but there's also a chance our model just happens to accurately predict delays well *by chance alone*; it would be nice if we could see if this is the case. \n",
    "\n",
    "To do this, we will use two metrics: McNemar's Test and the Cohen $\\kappa$ score. \n",
    "\n",
    "[McNemar's Test](https://en.wikipedia.org/wiki/McNemar's_test), which is commonly used for diagnostic tests in medical sciences, tells us if our model disagrees in the same way; for example, if it makes errors in a similar way with all data, it may be doing so by chance (accept null hypothesis), whereas if it disagrees with expected results  (reject null hypothesis), there are differences in how it disagrees (*i.e.* it's not due to chance . Here, we use the p-value to determine how strongly the associations are (generally, values of 0.05 are used to reject/accept the null hypothesis). [More here](https://machinelearningmastery.com/mcnemars-test-for-machine-learning/).\n",
    "\n",
    "The [Cohen $\\kappa$ score](https://en.wikipedia.org/wiki/Cohen's_kappa) is a measure of the agreement between two models that classify items. A score of around 1 tells us the models are in perfect agreement, and a score of around 0 means no agreement. \n",
    "\n",
    "Below, we will look at some toy models to give us a baseline for what types of results to expect:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Perfect agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results when match everything perfectly:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     48635\n",
      "         1.0       1.00      1.00      1.00     11354\n",
      "\n",
      "    accuracy                           1.00     59989\n",
      "   macro avg       1.00      1.00      1.00     59989\n",
      "weighted avg       1.00      1.00      1.00     59989\n",
      "\n",
      "[[48635     0]\n",
      " [    0 11354]]\n",
      "The p-value of the Mcnemar test (which I should use?) is 100.0%.\n",
      "The Cohen kappa score is 1.0.\n"
     ]
    }
   ],
   "source": [
    "#Proportion of total flights delayed\n",
    "\n",
    "print('Results when match everything perfectly:')\n",
    "\n",
    "# print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(Y_test, Y_test))\n",
    "\n",
    "conf_mat = confusion_matrix(Y_test, Y_test)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "\n",
    "mc_result = mcnemar(conf_mat, exact=True)\n",
    "\n",
    "print(f\"The p-value of the Mcnemar test (which I should use?) is {mc_result.pvalue*100:.1f}%.\")\n",
    "\n",
    "# So McNemar. Null hypothesis is that the model makes errors in the same proportion for both predictions. \n",
    "# If we reject null (p-val less than alpha, say, 5%), then the proportions are different in different ways, i.e.\n",
    "# there's a reason for the disagreement, i think\n",
    "\n",
    "print(f\"The Cohen kappa score is {cohen_kappa_score(Y_test, Y_test)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results when we just randomly guess which flights will be delayed:\n",
      "18.93% of all flights in the sample are delayed, based on our criteria for \"delayed\".\n",
      "0.4974411975528847\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.50      0.62     48635\n",
      "         1.0       0.19      0.50      0.27     11354\n",
      "\n",
      "    accuracy                           0.50     59989\n",
      "   macro avg       0.50      0.50      0.44     59989\n",
      "weighted avg       0.69      0.50      0.55     59989\n",
      "\n",
      "[[24177 24458]\n",
      " [ 5690  5664]]\n",
      "The probability that we got these results from random chance is 44.1%.\n",
      "The p-value of the Mcnemar test (which I should use?) is 0.0%.\n",
      "The Cohen kappa score is -0.0024693754933267087.\n"
     ]
    }
   ],
   "source": [
    "#Proportion of total flights delayed\n",
    "\n",
    "prop_delays = (Y_test.sum()/Y_test.count()).values[0]\n",
    "\n",
    "# If we just randomly guessed flights that were delayed, based on that calculated proportion, how well would we do?\n",
    "# See documentation; first parameter of 2 is like 'np.arange(2)'\n",
    "y_pred = np.random.choice(2, Y_test.shape[0])\n",
    "# y_pred = np.random.choice(2, size = y_test.shape[0], p=[1-prop_delays, prop_delays])\n",
    "\n",
    "print('Results when we just randomly guess which flights will be delayed:')\n",
    "\n",
    "print(f'{prop_delays*100:.2f}% of all flights in the sample are delayed, based on our criteria for \"delayed\".')\n",
    "\n",
    "print(accuracy_score(Y_test, y_pred))\n",
    "\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "conf_mat = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "oddsr, p = fisher_exact(conf_mat)\n",
    "\n",
    "print(f\"The probability that we got these results from random chance is {p*100:.1f}%.\")\n",
    "\n",
    "mc_result = mcnemar(conf_mat, exact=True)\n",
    "\n",
    "print(f\"The p-value of the Mcnemar test (which I should use?) is {mc_result.pvalue*100:.1f}%.\")\n",
    "\n",
    "print(f\"The Cohen kappa score is {cohen_kappa_score(Y_test, y_pred)}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Randomness based on proportion of true results (i.e. 80% chance of on-time; 20% chance of delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results when we bias the probabilities of our random guessing based on the actual proportion of delayed flights:\n",
      "18.93% of all flights in the sample are delayed, based on our criteria for \"delayed\".\n",
      "0.6923936054943406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81     48635\n",
      "         1.0       0.18      0.18      0.18     11354\n",
      "\n",
      "    accuracy                           0.69     59989\n",
      "   macro avg       0.50      0.50      0.50     59989\n",
      "weighted avg       0.69      0.69      0.69     59989\n",
      "\n",
      "[[39462  9173]\n",
      " [ 9280  2074]]\n",
      "The probability that we got these results from random chance is 14.6%.\n",
      "The p-value of the Mcnemar test (which I should use?) is 43.5%.\n",
      "The Cohen kappa score is -0.005963665439225219.\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.random.choice(2, size = Y_test.shape[0], p=[1-prop_delays, prop_delays])\n",
    "\n",
    "print('Results when we bias the probabilities of our random guessing based on the actual proportion of delayed flights:')\n",
    "\n",
    "print(f'{prop_delays*100:.2f}% of all flights in the sample are delayed, based on our criteria for \"delayed\".')\n",
    "\n",
    "print(accuracy_score(Y_test, y_pred))\n",
    "\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "conf_mat = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "oddsr, p = fisher_exact(conf_mat)\n",
    "\n",
    "print(f\"The probability that we got these results from random chance is {p*100:.1f}%.\")\n",
    "\n",
    "mc_result = mcnemar(conf_mat, exact=True)\n",
    "\n",
    "print(f\"The p-value of the Mcnemar test (which I should use?) is {mc_result.pvalue*100:.1f}%.\")\n",
    "\n",
    "print(f\"The Cohen kappa score is {cohen_kappa_score(Y_test, y_pred)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply different selection techniques to find out which one will be the best for our problems.\n",
    "\n",
    "- Original Features vs. PCA conponents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try Sparse PCA, since this is sparse data...\n",
    "\n",
    "def pca_processing(X, n):\n",
    "    '''Runs the MiniBatchSparsePCA on our data and returns the transformed features\n",
    "    Params:\n",
    "        - X: array of data features; assumed to be a sparse matrix (lots of 0s and 1s)\n",
    "        - n: Number of PCA components to fit with\n",
    "    Returns:\n",
    "        - x: array of transformed data with n features  \n",
    "        - x_columns: array of features names (just PCA number, in this case)\n",
    "        ''' \n",
    "    pca_trans = MiniBatchSparsePCA(n_components=n, batch_size=30, random_state=0)\n",
    "    pca_trans.fit(X)\n",
    "    x = pca_trans.transform(X)\n",
    "    x_columns = range(n)\n",
    "    return x, x_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something, based on the above results: remove ...things\n",
    "\n",
    "best model initially is mkt_unique_carrier, arr_hr and month\n",
    "\n",
    "**In cell below, commented out features will remain in the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.drop(list(feature_df.filter(regex = 'origin')), axis = 1, inplace=True) \n",
    "feature_df.drop(list(feature_df.filter(regex = 'dest')), axis = 1, inplace=True) \n",
    "\n",
    "# feature_df.drop(list(feature_df.filter(regex = 'mkt_unique_carrier')), axis = 1, inplace=True) \n",
    "feature_df.drop(list(feature_df.filter(regex = 'carrier_size')), axis = 1, inplace=True) \n",
    "\n",
    "feature_df.drop(list(feature_df.filter(regex = 'dep_hr')), axis = 1, inplace=True) \n",
    "feature_df.drop(list(feature_df.filter(regex = 'dep_time_of_day')), axis = 1, inplace=True) \n",
    "feature_df.drop(list(feature_df.filter(regex = 'arr_hr')), axis = 1, inplace=True) \n",
    "# feature_df.drop(list(feature_df.filter(regex = 'arr_time_of_day')), axis = 1, inplace=True) \n",
    "\n",
    "feature_df.drop(list(feature_df.filter(regex = 'month')), axis = 1, inplace=True) \n",
    "# feature_df.drop(list(feature_df.filter(regex = 'season')), axis = 1, inplace=True) \n",
    "feature_df.drop(list(feature_df.filter(regex = 'weekday')), axis = 1, inplace=True) \n",
    "\n",
    "feature_df.drop(list(feature_df.filter(regex = 'distance_cat')), axis = 1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mkt_unique_carrier_AA', 'mkt_unique_carrier_AS',\n",
       "       'mkt_unique_carrier_B6', 'mkt_unique_carrier_DL',\n",
       "       'mkt_unique_carrier_F9', 'mkt_unique_carrier_G4',\n",
       "       'mkt_unique_carrier_HA', 'mkt_unique_carrier_NK',\n",
       "       'mkt_unique_carrier_UA', 'mkt_unique_carrier_VX',\n",
       "       'mkt_unique_carrier_WN', 'arr_time_of_day_aft', 'arr_time_of_day_eve',\n",
       "       'arr_time_of_day_morn', 'season_aut', 'season_spr', 'season_sum',\n",
       "       'season_win'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see we have the desired features\n",
    "feature_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_219068/3332929115.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfc.fit(x_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature names  Importance\n",
      "0              0    0.443876\n",
      "1              1    0.313069\n",
      "2              2    0.141765\n",
      "4              4    0.062863\n",
      "3              3    0.038427\n",
      "Results when we remove origin/destination information:\n",
      "----------------------------------\n",
      "18.93% of all flights in the sample are delayed, based on our criteria for \"delayed\".\n",
      "The accuracy is 0.610.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.62      0.72     48491\n",
      "         1.0       0.26      0.56      0.35     11498\n",
      "\n",
      "    accuracy                           0.61     59989\n",
      "   macro avg       0.56      0.59      0.54     59989\n",
      "weighted avg       0.74      0.61      0.65     59989\n",
      "\n",
      "[[30134 18357]\n",
      " [ 5064  6434]]\n",
      "The probability that we got these results from random chance is 0.0%.\n",
      "The p-value of the Mcnemar test (which I should use?) is 0.0%.\n",
      "ROC-AUC Score: 0.624\n",
      "The Cohen kappa score is 0.12561533107024492.\n"
     ]
    }
   ],
   "source": [
    "X, x_columns = pca_processing(feature_df, 5)\n",
    "\n",
    "#--------------------------------\n",
    "##### Commenting out the above line and uncommenting the two lines below will run model without PCA \n",
    "\n",
    "# X = feature_df\n",
    "# x_columns = feature_df.columns\n",
    "#--------------------------------\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y_clas, test_size = 0.3)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, class_weight = 'balanced') # class_weights gives higher weight to our small number of delays\n",
    "rfc.fit(x_train, y_train)\n",
    "y_pred = rfc.predict(x_test)\n",
    "\n",
    "feature_val = rfc.feature_importances_\n",
    "feature_name = x_columns\n",
    "# print(feature_name)\n",
    "\n",
    "features = pd.DataFrame({\"Feature names\": pd.Series(dtype='str'),\n",
    "                         \"Importance\": pd.Series(dtype='int')})\n",
    "features['Feature names'] = feature_name\n",
    "features['Importance'] = feature_val\n",
    "print(features.sort_values(by=['Importance'], ascending=False).head())\n",
    "\n",
    "print('Results when we remove origin/destination information:')\n",
    "\n",
    "print('----------------------------------')\n",
    "\n",
    "print(f'{prop_delays*100:.2f}% of all flights in the sample are delayed, based on our criteria for \"delayed\".')\n",
    "\n",
    "print(f'The accuracy is {accuracy_score(y_test, y_pred):.3f}.')\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "oddsr, p = fisher_exact(conf_mat)\n",
    "\n",
    "print(f\"The probability that we got these results from random chance is {p*100:.1f}%.\")\n",
    "\n",
    "mc_result = mcnemar(conf_mat, exact=True)\n",
    "\n",
    "print(f\"The p-value of the Mcnemar test (which I should use?) is {mc_result.pvalue*100:.1f}%.\")\n",
    "\n",
    "print(f'ROC-AUC Score: {roc_auc_score(y_test, rfc.predict_proba(x_test)[:,1]):.3f}')\n",
    "\n",
    "print(f\"The Cohen kappa score is {cohen_kappa_score(y_test, y_pred)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.610.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.62      0.72     48491\n",
      "         1.0       0.26      0.56      0.35     11498\n",
      "\n",
      "    accuracy                           0.61     59989\n",
      "   macro avg       0.56      0.59      0.54     59989\n",
      "weighted avg       0.74      0.61      0.65     59989\n",
      "\n",
      "[[30134 18357]\n",
      " [ 5064  6434]]\n",
      "The probability that we got these results from random chance is 0.0%.\n",
      "The p-value of the Mcnemar test (which I should use?) is 0.0%.\n",
      "ROC-AUC Score: 0.623\n",
      "The Cohen kappa score is 0.12561533107024492.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dtc = DecisionTreeClassifier(class_weight = 'balanced') # class_weights gives higher weight to our small number of delays\n",
    "dtc.fit(x_train, y_train)\n",
    "y_pred = dtc.predict(x_test)\n",
    "\n",
    "print(f'The accuracy is {accuracy_score(y_test, y_pred):.3f}.')\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "oddsr, p = fisher_exact(conf_mat)\n",
    "\n",
    "print(f\"The probability that we got these results from random chance is {p*100:.1f}%.\")\n",
    "\n",
    "mc_result = mcnemar(conf_mat, exact=True)\n",
    "\n",
    "print(f\"The p-value of the Mcnemar test (which I should use?) is {mc_result.pvalue*100:.1f}%.\")\n",
    "\n",
    "print(f'ROC-AUC Score: {roc_auc_score(y_test, dtc.predict_proba(x_test)[:,1]):.3f}')\n",
    "\n",
    "print(f\"The Cohen kappa score is {cohen_kappa_score(y_test, y_pred)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgbrf = XGBRFClassifier(colsample_bynode=0.8, max_depth= 40, eta= 1,  objective =  'binary:logistic' ,  subsample =0.7,  max_delta_step = 1, \\\n",
    "      tree_method =  'hist' ,  num_parallel_tree = 150,   scale_pos_weight = 4).fit(x_train, y_train)\n",
    "# dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "# dtest = xgb.DMatrix(x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_round = 10\n",
    "bst = xgbrf.fit( x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.617.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.63      0.73     48491\n",
      "         1.0       0.26      0.55      0.35     11498\n",
      "\n",
      "    accuracy                           0.62     59989\n",
      "   macro avg       0.56      0.59      0.54     59989\n",
      "weighted avg       0.74      0.62      0.66     59989\n",
      "\n",
      "[[30687 17804]\n",
      " [ 5188  6310]]\n",
      "The probability that we got these results from random chance is 0.0%.\n",
      "The p-value of the Mcnemar test (which I should use?) is 0.0%.\n",
      "ROC-AUC Score: 0.623\n",
      "The Cohen kappa score is 0.12804102112546378.\n"
     ]
    }
   ],
   "source": [
    "ypred = bst.predict(x_test)\n",
    "y_pred=np.round(ypred)\n",
    "\n",
    "print(f'The accuracy is {accuracy_score(y_test, y_pred):.3f}.')\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "oddsr, p = fisher_exact(conf_mat)\n",
    "\n",
    "print(f\"The probability that we got these results from random chance is {p*100:.1f}%.\")\n",
    "\n",
    "mc_result = mcnemar(conf_mat, exact=True)\n",
    "\n",
    "print(f\"The p-value of the Mcnemar test (which I should use?) is {mc_result.pvalue*100:.1f}%.\")\n",
    "\n",
    "print(f'ROC-AUC Score: {roc_auc_score(y_test, dtc.predict_proba(x_test)[:,1]):.3f}')\n",
    "\n",
    "print(f\"The Cohen kappa score is {cohen_kappa_score(y_test, y_pred)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('lhl_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6701a37b50cf3fe420c1c168326b5df267cda9c0faa66bc5c6c8150dd9165b51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
